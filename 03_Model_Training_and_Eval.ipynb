{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e946638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Loading and Full Preprocessing Pipeline ---\n",
      "Raw Data Loaded.\n",
      "Target variable 'is_default' defined and data filtered.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # import traditional libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression # The first model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve # for model evaluation\n",
    "\n",
    "# Imbalanced Learn for handling imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Supress warnings for clear output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"--- Starting Data Loading and Full Preprocessing Pipeline ---\")\n",
    "\n",
    "# Import Data\n",
    "data_path = 'data/accepted_2007_to_2018q4.csv/accepted_2007_to_2018q4.csv'\n",
    "df_accepted = pd.read_csv(data_path, low_memory=False)\n",
    "print(\"Raw Data Loaded.\")\n",
    "\n",
    "# Target Mapping\n",
    "target_mapping = {\n",
    "    \"Charged Off\": 1,\n",
    "    \"Default\": 1,\n",
    "    \"Late (31-120 days)\": 1,\n",
    "    \"Does not meet the credit policy. Status:Charged Off\": 1,\n",
    "    \"Fully Paid\": 0,\n",
    "    \"Does not meet the credit policy. Status:Fully Paid\": 0\n",
    "} \n",
    "\n",
    "final_statuses_for_model = list(target_mapping.keys())\n",
    "df_filtered = df_accepted[df_accepted['loan_status'].isin(final_statuses_for_model)].copy()\n",
    "df_filtered['is_default'] = df_filtered['loan_status'].map(target_mapping)\n",
    "y = df_filtered['is_default']\n",
    "print(\"Target variable 'is_default' defined and data filtered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6df30c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Columns Dropped.\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    # 1. Identifiers and Unnecessary Columns\n",
    "    'id',\n",
    "    'member_id',\n",
    "    'url',\n",
    "    'desc',\n",
    "    'loan_status',\n",
    "    'title',\n",
    "    'zip_code',\n",
    "    'emp_title',\n",
    "\n",
    "    # 2. Data Leakage: Information that would not be available at the time of loan application, only available after the loan is issued or defaulted\n",
    "    'funded_amnt',\n",
    "    'funded_amnt_inv',\n",
    "    'pymnt_plan',\n",
    "    'out_prncp',\n",
    "    'out_prncp_inv',\n",
    "    'total_pymnt',\n",
    "    'total_pymnt_inv',\n",
    "    'total_rec_prncp',\n",
    "    'total_rec_int',\n",
    "    'total_rec_late_fee',\n",
    "    'recoveries',\n",
    "    'collection_recovery_fee',\n",
    "    'last_pymnt_d',\n",
    "    'last_pymnt_amnt',\n",
    "    'next_pymnt_d',\n",
    "    'last_credit_pull_d',\n",
    "    'last_fico_range_high',\n",
    "    'last_fico_range_low',\n",
    "\n",
    "    # 3. Hardship and Debt Settlement Information (outcomes of distress not related to the original loan application)\n",
    "    'hardship_flag',\n",
    "    'hardship_type',\n",
    "    'hardship_reason',\n",
    "    'hardship_status',\n",
    "    'deferral_term',\n",
    "    'hardship_amount',\n",
    "    'hardship_start_date',\n",
    "    'hardship_end_date',\n",
    "    'payment_plan_start_date',\n",
    "    'hardship_length',\n",
    "    'hardship_dpd',\n",
    "    'hardship_loan_status',\n",
    "    'orig_projected_additional_accrued_interest',\n",
    "    'hardship_payoff_balance_amount',\n",
    "    'hardship_last_payment_amount',\n",
    "    'debt_settlement_flag',\n",
    "    'debt_settlement_flag_date',\n",
    "    'settlement_status',\n",
    "    'settlement_date',\n",
    "    'settlement_amount',\n",
    "    'settlement_percentage',\n",
    "    'settlement_term',\n",
    "\n",
    "    # 4. Joint/Secondary Borrower Information (not relevant for single borrower analysis)\n",
    "    'annual_inc_joint',\n",
    "    'dti_joint',\n",
    "    'verification_status_joint',\n",
    "    'revol_bal_joint',\n",
    "    'sec_app_fico_range_low',\n",
    "    'sec_app_fico_range_high',\n",
    "    'sec_app_earliest_cr_line',\n",
    "    'sec_app_inq_last_6mths',\n",
    "    'sec_app_mort_acc',\n",
    "    'sec_app_open_acc',\n",
    "    'sec_app_revol_util',\n",
    "    'sec_app_open_act_il',\n",
    "    'sec_app_num_rev_accts',\n",
    "    'sec_app_chargeoff_within_12_mths',\n",
    "    'sec_app_collections_12_mths_ex_med',\n",
    "    'sec_app_mths_since_last_major_derog',\n",
    "\n",
    "    # 5. Columns with High Missing Values or Irrelevant Information\n",
    "    'mths_since_last_delinq',\n",
    "    'policy_code'\n",
    "]\n",
    "\n",
    "existing_cols_to_drop = [col for col in columns_to_drop if col in df_filtered.columns]\n",
    "X = df_filtered.drop(columns=existing_cols_to_drop + ['is_default'], errors='ignore')\n",
    "print(\"Initial Columns Dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06963310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting full preprocessing on X_processed ---\n"
     ]
    }
   ],
   "source": [
    "# Apply all preprocessing steps from 02_Data_Preprocessing.ipynb\n",
    "\n",
    "X_processed = X.copy()\n",
    "print(\"--- Starting full preprocessing on X_processed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a4272c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Data Feature Engineering\n",
      "Processing 'issue_d' column...\n",
      "All dates in 'issue_d' were successfully parsed.\n",
      "'issue_d' column processed and dropped.\n",
      "Processing 'earliest_cr_line' column...\n",
      "Warning: 29 unparseable dates found in 'earliest_cr_line'. These will be dropped.\n",
      "Credit history length calculated successfully.\n",
      "'earliest_cr_line' column processed and dropped.\n",
      "'issue_d_dt' intermediate column dropped.\n"
     ]
    }
   ],
   "source": [
    "print(\"1. Data Feature Engineering\")\n",
    "\n",
    "if 'issue_d' in X_processed.columns:\n",
    "    print(\"Processing 'issue_d' column...\")\n",
    "\n",
    "    #Convert 'issue_d' to datetime\n",
    "    X_processed['issue_d_dt'] = pd.to_datetime(X_processed['issue_d'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "    # Check for unparseable dates\n",
    "    unparseable_dates = X_processed['issue_d_dt'].isnull().sum()\n",
    "    if unparseable_dates > 0:\n",
    "        print(f\"Warning: {unparseable_dates} unparseable dates found in 'issue_d'. These will be dropped.\")\n",
    "    else:\n",
    "        print(\"All dates in 'issue_d' were successfully parsed.\")\n",
    "\n",
    "    X_processed['issue_month'] = X_processed['issue_d_dt'].dt.month\n",
    "    X_processed['issue_year'] = X_processed['issue_d_dt'].dt.year\n",
    "    X_processed['issue_dayofweek'] = X_processed['issue_d_dt'].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "    X_processed.drop(columns=['issue_d'], inplace=True, errors='ignore')\n",
    "    print(\"'issue_d' column processed and dropped.\")\n",
    "else:\n",
    "    print(\"'issue_d' column not found in the DataFrame. Skipping processing.\")\n",
    "\n",
    "if 'earliest_cr_line' in X_processed.columns:\n",
    "    print(\"Processing 'earliest_cr_line' column...\")\n",
    "    X_processed['earliest_cr_line_dt'] = pd.to_datetime(X_processed['earliest_cr_line'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "    unparseable_cr_lines = X_processed['earliest_cr_line_dt'].isnull().sum()\n",
    "    if unparseable_cr_lines > 0:\n",
    "        print(f\"Warning: {unparseable_cr_lines} unparseable dates found in 'earliest_cr_line'. These will be dropped.\")\n",
    "    else:\n",
    "        print(\"All dates in 'earliest_cr_line' were successfully parsed.\")\n",
    "    \n",
    "    if 'issue_d_dt' in X_processed.columns:\n",
    "        X_processed['credit_history_length_months'] = ((X_processed['issue_d_dt'] - X_processed['earliest_cr_line_dt']).dt.days / 30.4375).astype(float) # Average days per month ; allow for NaN\n",
    "\n",
    "        # Handle negative values, if somehow the earliest credit line is after the issue date\n",
    "        X_processed.loc[X_processed['credit_history_length_months'] < 0, 'credit_history_length_months'] = np.nan\n",
    "        print(\"Credit history length calculated successfully.\")\n",
    "    else:\n",
    "        print(\"Warning: 'issue_d_dt' not found. Cannot calculate credit history length.\")\n",
    "    \n",
    "    X_processed.drop(columns=['earliest_cr_line', 'earliest_cr_line_dt'], inplace=True, errors='ignore')\n",
    "    print(\"'earliest_cr_line' column processed and dropped.\")\n",
    "\n",
    "else: # Almost sure this is not needed, but keeping for safety, if the column has already been split or removed (wasnt used in the original code)\n",
    "    print(\"'earliest_cr_line' column not found in the DataFrame.\")\n",
    "    \n",
    "    earliest_cr_line_encoded_cols = [col for col in X_processed.columns if col.startswith('earliest_cr_line_')] # This will be empty if the column was not processed\n",
    "\n",
    "    if earliest_cr_line_encoded_cols:\n",
    "        print(f\"Warning: The following columns were not processed due to missing 'earliest_cr_line': {earliest_cr_line_encoded_cols}\")\n",
    "\n",
    "        date_from_col_name_map = {\n",
    "            col: pd.to_datetime(col.replace('earliest_cr_line_', '').format('%b-%Y'), errors='coerce')\n",
    "            for col in earliest_cr_line_encoded_cols\n",
    "        }\n",
    "\n",
    "        X_processed['credit_history_length_months'] = np.nan\n",
    "        print(\"Calculating credit history length from encoded columns...\")\n",
    "\n",
    "        issue_d_dt = X_processed.get('issue_d_dt')\n",
    "\n",
    "        for index, row_series in X_processed.iterrows():\n",
    "            issue_date = issue_d_dt[index] if issue_d_dt is not None else pd.NaT\n",
    "\n",
    "            if pd.isna(issue_date):\n",
    "                continue\n",
    "\n",
    "            earliest_date_for_row = None\n",
    "\n",
    "            for col in earliest_cr_line_encoded_cols:\n",
    "                if row_series[col] == 1:\n",
    "                    earliest_date_for_row = date_from_col_name_map[col]\n",
    "                    break\n",
    "            if earliest_date_for_row and pd.notna(earliest_date_for_row):\n",
    "                days_diff = (issue_date - earliest_date_for_row).days\n",
    "                X_processed.loc[index, 'credit_history_length_months'] = days_diff / 30.4375  # Average days per month\n",
    "                # handle negative values\n",
    "                if X_processed.loc[index, 'credit_history_length_months'] < 0:\n",
    "                    X_processed.loc[index, 'credit_history_length_months'] = np.nan\n",
    "        \n",
    "        X_processed.drop(columns=earliest_cr_line_encoded_cols, inplace=True, errors='ignore')\n",
    "        print(\"Credit history length calculated from encoded columns.\")\n",
    "    else:\n",
    "        print(\"No encoded columns for 'earliest_cr_line' found. Cannot calculate credit history length.\")\n",
    "\n",
    "if 'issue_d_dt' in X_processed.columns:\n",
    "    X_processed.drop(columns=['issue_d_dt'], inplace=True, errors='ignore')\n",
    "    print(\"'issue_d_dt' intermediate column dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb38fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Numerical Preprocessing Lop1p and Scaling\n",
      "\n",
      "--- Applying Log1p Transformation to Skewed/Sparse Numerical Features ---\n",
      "Log1p transformation applied to column: loan_amnt\n",
      "Log1p transformation applied to column: installment\n",
      "Log1p transformation applied to column: annual_inc\n",
      "Warning: Column 'dti' contains negative values. Skipping log transformation.\n",
      "Log1p transformation applied to column: delinq_2yrs\n",
      "Log1p transformation applied to column: inq_last_6mths\n",
      "Log1p transformation applied to column: open_acc\n",
      "Log1p transformation applied to column: pub_rec\n",
      "Log1p transformation applied to column: revol_bal\n",
      "Log1p transformation applied to column: revol_util\n",
      "Log1p transformation applied to column: total_acc\n",
      "Log1p transformation applied to column: collections_12_mths_ex_med\n",
      "Log1p transformation applied to column: mths_since_last_major_derog\n",
      "Log1p transformation applied to column: acc_now_delinq\n",
      "Log1p transformation applied to column: tot_coll_amt\n",
      "Log1p transformation applied to column: tot_cur_bal\n",
      "Log1p transformation applied to column: open_acc_6m\n",
      "Log1p transformation applied to column: open_act_il\n",
      "Log1p transformation applied to column: open_il_12m\n",
      "Log1p transformation applied to column: open_il_24m\n",
      "Log1p transformation applied to column: mths_since_rcnt_il\n",
      "Log1p transformation applied to column: total_bal_il\n",
      "Log1p transformation applied to column: il_util\n",
      "Log1p transformation applied to column: open_rv_12m\n",
      "Log1p transformation applied to column: open_rv_24m\n",
      "Log1p transformation applied to column: max_bal_bc\n",
      "Log1p transformation applied to column: all_util\n",
      "Log1p transformation applied to column: total_rev_hi_lim\n",
      "Log1p transformation applied to column: inq_fi\n",
      "Log1p transformation applied to column: total_cu_tl\n",
      "Log1p transformation applied to column: inq_last_12m\n",
      "Log1p transformation applied to column: acc_open_past_24mths\n",
      "Log1p transformation applied to column: avg_cur_bal\n",
      "Log1p transformation applied to column: bc_open_to_buy\n",
      "Log1p transformation applied to column: bc_util\n",
      "Log1p transformation applied to column: chargeoff_within_12_mths\n",
      "Log1p transformation applied to column: delinq_amnt\n",
      "Log1p transformation applied to column: mo_sin_old_il_acct\n",
      "Log1p transformation applied to column: mo_sin_old_rev_tl_op\n",
      "Log1p transformation applied to column: mo_sin_rcnt_rev_tl_op\n",
      "Log1p transformation applied to column: mo_sin_rcnt_tl\n",
      "Log1p transformation applied to column: mort_acc\n",
      "Log1p transformation applied to column: mths_since_recent_bc\n",
      "Log1p transformation applied to column: mths_since_recent_bc_dlq\n",
      "Log1p transformation applied to column: mths_since_recent_inq\n",
      "Log1p transformation applied to column: mths_since_recent_revol_delinq\n",
      "Log1p transformation applied to column: num_accts_ever_120_pd\n",
      "Log1p transformation applied to column: num_actv_bc_tl\n",
      "Log1p transformation applied to column: num_actv_rev_tl\n",
      "Log1p transformation applied to column: num_bc_sats\n",
      "Log1p transformation applied to column: num_bc_tl\n",
      "Log1p transformation applied to column: num_il_tl\n",
      "Log1p transformation applied to column: num_op_rev_tl\n",
      "Log1p transformation applied to column: num_rev_accts\n",
      "Log1p transformation applied to column: num_rev_tl_bal_gt_0\n",
      "Log1p transformation applied to column: num_sats\n",
      "Log1p transformation applied to column: num_tl_120dpd_2m\n",
      "Log1p transformation applied to column: num_tl_30dpd\n",
      "Log1p transformation applied to column: num_tl_90g_dpd_24m\n",
      "Log1p transformation applied to column: num_tl_op_past_12m\n",
      "Log1p transformation applied to column: pct_tl_nvr_dlq\n",
      "Log1p transformation applied to column: percent_bc_gt_75\n",
      "Log1p transformation applied to column: pub_rec_bankruptcies\n",
      "Log1p transformation applied to column: tax_liens\n",
      "Log1p transformation applied to column: tot_hi_cred_lim\n",
      "Log1p transformation applied to column: total_bal_ex_mort\n",
      "Log1p transformation applied to column: total_bc_limit\n",
      "Log1p transformation applied to column: total_il_high_credit_limit\n"
     ]
    }
   ],
   "source": [
    "print(\"2. Numerical Preprocessing Lop1p and Scaling\")\n",
    "log_transform_columns = [\n",
    "    'loan_amnt', 'installment', 'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths',\n",
    "    'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc',\n",
    "    'collections_12_mths_ex_med', 'mths_since_last_major_derog', 'acc_now_delinq',\n",
    "    'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m', 'open_act_il', 'open_il_12m',\n",
    "    'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m',\n",
    "    'open_rv_24m', 'max_bal_bc', 'all_util', 'total_rev_hi_lim', 'inq_fi',\n",
    "    'total_cu_tl', 'inq_last_12m', 'acc_open_past_24mths', 'avg_cur_bal',\n",
    "    'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt',\n",
    "    'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl',\n",
    "    'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n",
    "    'mths_since_recent_inq', 'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd',\n",
    "    'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl',\n",
    "    'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats',\n",
    "    'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m',\n",
    "    'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens',\n",
    "    'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit' # These were determined in EDA #1, with the observations of the distributions\n",
    "]\n",
    "\n",
    "log_transform_columns = [col for col in log_transform_columns if col in X_processed.columns and col not in ['int_rate', 'fico_range_low', 'fico_range_high']]\n",
    "\n",
    "print(\"\\n--- Applying Log1p Transformation to Skewed/Sparse Numerical Features ---\")\n",
    "for col in log_transform_columns:\n",
    "    if col in X_processed.columns:\n",
    "        if (X_processed[col] < 0).any().any(): # check for negative values\n",
    "            print(f\"Warning: Column '{col}' contains negative values. Skipping log transformation.\")\n",
    "        else: \n",
    "            # Must Fill NaN values with 0 before log transformation\n",
    "            # For now, we can handle them later\n",
    "            X_processed[col] = np.log1p(X_processed[col])\n",
    "            print(f\"Log1p transformation applied to column: {col}\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' not found in DataFrame. Skipping log transformation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa5058a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scaling Numerical Features ---\n",
      "Scaling completed.\n"
     ]
    }
   ],
   "source": [
    "cols_to_scale = X_processed.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"\\n--- Scaling Numerical Features ---\") # for the final model, StandardScaler is used only on the training set\n",
    "X_processed[cols_to_scale] = scaler.fit_transform(X_processed[cols_to_scale])\n",
    "print(\"Scaling completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b6b5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Missing Value Handling\n",
      "\n",
      "Numerical columns with NaN values: ['annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_util', 'total_acc', 'collections_12_mths_ex_med', 'mths_since_last_major_derog', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m', 'open_act_il', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m', 'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq', 'mths_since_recent_inq', 'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit', 'credit_history_length_months']\n",
      "Imputed 'annual_inc' with mode value: -0.14\n",
      "Imputed 'dti' with mode value: -0.03\n",
      "Imputed 'delinq_2yrs' with mode value: -0.45\n",
      "Imputed 'inq_last_6mths' with mode value: -0.80\n",
      "Imputed 'mths_since_last_record' with mode value: 0.02\n",
      "Imputed 'open_acc' with mode value: -0.33\n",
      "Imputed 'pub_rec' with mode value: -0.43\n",
      "Imputed 'revol_util' with mode value: -5.23\n",
      "Imputed 'total_acc' with mode value: -0.21\n",
      "Imputed 'collections_12_mths_ex_med' with mode value: -0.12\n",
      "Imputed 'mths_since_last_major_derog' with mode value: 0.29\n",
      "Imputed 'acc_now_delinq' with mode value: -0.07\n",
      "Imputed 'tot_coll_amt' with mode value: -0.41\n",
      "Imputed 'tot_cur_bal' with mode value: -8.67\n",
      "Imputed 'open_acc_6m' with mode value: -1.06\n",
      "Imputed 'open_act_il' with mode value: -0.67\n",
      "Imputed 'open_il_12m' with mode value: -0.92\n",
      "Imputed 'open_il_24m' with mode value: -0.25\n",
      "Imputed 'mths_since_rcnt_il' with mode value: -1.09\n",
      "Imputed 'total_bal_il' with mode value: -2.76\n",
      "Imputed 'il_util' with mode value: 0.44\n",
      "Imputed 'open_rv_12m' with mode value: -1.20\n",
      "Imputed 'open_rv_24m' with mode value: -0.73\n",
      "Imputed 'max_bal_bc' with mode value: -4.78\n",
      "Imputed 'all_util' with mode value: 0.34\n",
      "Imputed 'total_rev_hi_lim' with mode value: -1.00\n",
      "Imputed 'inq_fi' with mode value: -0.90\n",
      "Imputed 'total_cu_tl' with mode value: -0.81\n",
      "Imputed 'inq_last_12m' with mode value: -1.38\n",
      "Imputed 'acc_open_past_24mths' with mode value: -0.32\n",
      "Imputed 'avg_cur_bal' with mode value: -7.49\n",
      "Imputed 'bc_open_to_buy' with mode value: -4.25\n",
      "Imputed 'bc_util' with mode value: -4.66\n",
      "Imputed 'chargeoff_within_12_mths' with mode value: -0.09\n",
      "Imputed 'delinq_amnt' with mode value: -0.06\n",
      "Imputed 'mo_sin_old_il_acct' with mode value: 0.20\n",
      "Imputed 'mo_sin_old_rev_tl_op' with mode value: -0.30\n",
      "Imputed 'mo_sin_rcnt_rev_tl_op' with mode value: -1.18\n",
      "Imputed 'mo_sin_rcnt_tl' with mode value: -1.01\n",
      "Imputed 'mort_acc' with mode value: -1.05\n",
      "Imputed 'mths_since_recent_bc' with mode value: -1.24\n",
      "Imputed 'mths_since_recent_bc_dlq' with mode value: -0.24\n",
      "Imputed 'mths_since_recent_inq' with mode value: -1.16\n",
      "Imputed 'mths_since_recent_revol_delinq' with mode value: -0.89\n",
      "Imputed 'num_accts_ever_120_pd' with mode value: -0.50\n",
      "Imputed 'num_actv_bc_tl' with mode value: -0.08\n",
      "Imputed 'num_actv_rev_tl' with mode value: -0.35\n",
      "Imputed 'num_bc_sats' with mode value: -0.48\n",
      "Imputed 'num_bc_tl' with mode value: -0.52\n",
      "Imputed 'num_il_tl' with mode value: -0.49\n",
      "Imputed 'num_op_rev_tl' with mode value: -0.35\n",
      "Imputed 'num_rev_accts' with mode value: -0.42\n",
      "Imputed 'num_rev_tl_bal_gt_0' with mode value: -0.34\n",
      "Imputed 'num_sats' with mode value: -0.34\n",
      "Imputed 'num_tl_120dpd_2m' with mode value: -0.03\n",
      "Imputed 'num_tl_30dpd' with mode value: -0.06\n",
      "Imputed 'num_tl_90g_dpd_24m' with mode value: -0.23\n",
      "Imputed 'num_tl_op_past_12m' with mode value: -0.51\n",
      "Imputed 'pct_tl_nvr_dlq' with mode value: 0.61\n",
      "Imputed 'percent_bc_gt_75' with mode value: -1.69\n",
      "Imputed 'pub_rec_bankruptcies' with mode value: -0.37\n",
      "Imputed 'tax_liens' with mode value: -0.17\n",
      "Imputed 'tot_hi_cred_lim' with mode value: -2.05\n",
      "Imputed 'total_bal_ex_mort' with mode value: -10.89\n",
      "Imputed 'total_bc_limit' with mode value: -6.79\n",
      "Imputed 'total_il_high_credit_limit' with mode value: -2.61\n",
      "Imputed 'credit_history_length_months' with mode value: -0.56\n"
     ]
    }
   ],
   "source": [
    "print(\"3. Missing Value Handling\")\n",
    "numerical_cols_nans = X_processed.select_dtypes(include=[np.number]).columns[X_processed.select_dtypes(include=[np.number]).isnull().any()].tolist()\n",
    "print(f\"\\nNumerical columns with NaN values: {numerical_cols_nans}\")\n",
    "for col in numerical_cols_nans:\n",
    "    mode_val = X_processed[col].mode()[0]  # Get the mode value\n",
    "    # X_processed[col].fillna(median_val, inplace=True)\n",
    "    X_processed[col] = X_processed[col].fillna(mode_val) # This is the same as above, but more explicit and should get rid of the warning\n",
    "    print(f\"Imputed '{col}' with mode value: {mode_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "068c4faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Check for Missing Values ---\n",
      "80401\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Final Check for Missing Values ---\")\n",
    "print(X_processed.isnull().sum().sum())  # Should be 0 if all NaNs are filled, but isn't since we have some categorical columns with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e62bb28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorical features encoded successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_features_to_encode = X_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "if categorical_features_to_encode:\n",
    "    encoder_one_hot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # handle_unknown='ignore' to avoid errors with unseen categories\n",
    "\n",
    "    encoded_features = encoder_one_hot.fit_transform(X_processed[categorical_features_to_encode])\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoder_one_hot.get_feature_names_out(categorical_features_to_encode), index=X_processed.index)\n",
    "\n",
    "    X_processed = X_processed.drop(columns=categorical_features_to_encode)\n",
    "    X_processed = pd.concat([X_processed, encoded_df], axis=1)\n",
    "    print(\"\\nCategorical features encoded successfully.\")\n",
    "else:\n",
    "    print(\"\\nNo categorical features to encode.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcf2a19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total remaining missing values in X_processed: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTotal remaining missing values in X_processed:\", X_processed.isnull().sum().sum())  # Should be 0 if all NaNs are filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a2cdcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Categorical Encoding\n",
      "    Categorical Encoding...\n",
      "Categorical features encoded.\n"
     ]
    }
   ],
   "source": [
    "print(\"4. Categorical Encoding\")\n",
    "\n",
    "print(\"    Categorical Encoding...\")\n",
    "grade_order = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "sub_grade_order = ['A1', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2', 'B3', 'B4', 'B5', 'C1', 'C2', 'C3', 'C4', 'C5', 'D1', 'D2', 'D3', 'D4', 'D5', 'E1', 'E2', 'E3', 'E4', 'E5', 'F1', 'F2', 'F3', 'F4', 'F5', 'G1', 'G2', 'G3', 'G4', 'G5']\n",
    "emp_length_order = ['< 1 year', '1 year', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', '8 years', '9 years', '10+ years', 'Missing']\n",
    "\n",
    "if 'grade' in X_processed.columns:\n",
    "    encoder_grade = OrdinalEncoder(categories=[grade_order], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    X_processed['grade_encoded'] = encoder_grade.fit_transform(X_processed[['grade']])\n",
    "    X_processed.drop(columns=['grade'], inplace=True)\n",
    "if 'sub_grade' in X_processed.columns:\n",
    "    encoder_sub_grade = OrdinalEncoder(categories=[sub_grade_order], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    X_processed['sub_graded_encoded'] = encoder_sub_grade.fit_transform(X_processed[['sub_grade']])\n",
    "    X_processed.drop(columns=['sub_grade'], inplace=True)\n",
    "if 'emp_length' in X_processed.columns:\n",
    "    encoder_emp_length = OrdinalEncoder(categories=[emp_length_order], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    X_processed['emp_length_encoded'] = encoder_emp_length.fit_transform(X_processed[['emp_length']])\n",
    "    X_processed.drop(columns=['emp_length'], inplace=True)\n",
    "\n",
    "one_hot_cols = X_processed.select_dtypes(include='object').columns.tolist()\n",
    "if one_hot_cols:\n",
    "    encoder_one_hot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    encoded_features = encoder_one_hot.fit_transform(X_processed[one_hot_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoder_one_hot.get_feature_names_out(one_hot_cols), index=X_processed.index)\n",
    "    X_processed = X_processed.drop(columns=one_hot_cols)\n",
    "    X_processed = pd.concat([X_processed, encoded_df], axis=1)\n",
    "    \n",
    "print(\"Categorical features encoded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eddc7c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing Complete. Final X_processed state --- \n",
      "X_processed shape: (1369566, 212), Dtypes unique: 1\n",
      "   loan_amnt  int_rate  ...  disbursement_method_Cash  disbursement_method_DirectPay\n",
      "0  -1.692855  0.148594  ...                       1.0                            0.0\n",
      "1   1.069896 -0.269458  ...                       1.0                            0.0\n",
      "2   0.767081 -0.522380  ...                       1.0                            0.0\n",
      "4  -0.171057  1.916955  ...                       1.0                            0.0\n",
      "5   0.028245  0.033630  ...                       1.0                            0.0\n",
      "\n",
      "[5 rows x 212 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Preprocessing Complete. Final X_processed state --- \")\n",
    "print(f\"X_processed shape: {X_processed.shape}, Dtypes unique: {X_processed.dtypes.nunique()}\")\n",
    "print(X_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c98f6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Data into Training and Test Set\n",
      "Original Data Shape: (1369566, 212)\n",
      "Training data (X_train) shape: (1095652, 212)\n",
      "Testing data (X_test) shape: (273914, 212)\n",
      "\n",
      "Original Target Distribution:\n",
      " is_default\n",
      "0    0.78765\n",
      "1    0.21235\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Training Target Distribution:\n",
      " is_default\n",
      "0    0.787651\n",
      "1    0.212349\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Testing Target Distribution:\n",
      " is_default\n",
      "0    0.787649\n",
      "1    0.212351\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting Data into Training and Test Set\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y,\n",
    "    test_size=0.2, # All this comes from 00_SciKitLearn_Intro\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Original Data Shape: {X_processed.shape}\")\n",
    "print(f\"Training data (X_train) shape: {X_train.shape}\")\n",
    "print(f\"Testing data (X_test) shape: {X_test.shape}\")\n",
    "\n",
    "print(\"\\nOriginal Target Distribution:\\n\", y.value_counts(normalize=True))\n",
    "print(\"\\nTraining Target Distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"\\nTesting Target Distribution:\\n\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8ff10",
   "metadata": {},
   "source": [
    "## Data Splitting and Stratification\n",
    "\n",
    "The data was split into training and testing sets using `train_test_split` for two primary reasons:\n",
    "\n",
    "1.  **Preventing Overfitting:** The training set (`X_train`, `y_train`) is used to train the machine learning model. The model learns patterns from this data. If we were to evaluate the model on the same data it was trained on, it might perform exceptionally well simply by memorizing the training examples, rather than learning generalizable patterns. This is known as overfitting.\n",
    "2.  **Evaluating on Unseen Data:** The testing set (`X_test`, `y_test`) is held back and not used during the training phase. After the model is trained, it is evaluated on this unseen data. This provides a more realistic assessment of how the model will perform on new, real-world data it has never encountered before.\n",
    "\n",
    "The `stratify=y` parameter was used during the split because the target variable (`y`, representing loan default) is imbalanced. Stratification ensures that the proportion of the target classes (default vs. non-default) is the same in both the training and testing sets as it is in the original dataset. This is crucial for training and evaluating models on imbalanced data, as it prevents scenarios where one split might have a disproportionately high or low number of the minority class (defaults), leading to biased model training or evaluation.\n",
    "\n",
    "As confirmed by the output from the previous cell, the class distributions in `y_train` and `y_test` are indeed very close to the original `y` distribution, indicating that the stratification was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b088cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Handling Class Imbalance with SMOTE ---\n",
      "Original training target distribution:\n",
      " <bound method IndexOpsMixin.value_counts of 980969     0\n",
      "1933994    0\n",
      "874735     0\n",
      "399241     0\n",
      "1787576    0\n",
      "          ..\n",
      "1903010    1\n",
      "1317076    1\n",
      "174811     0\n",
      "87647      0\n",
      "2135287    1\n",
      "Name: is_default, Length: 1095652, dtype: int64>\n",
      "\n",
      "Resampled training target distribution:\n",
      " is_default\n",
      "0    862991\n",
      "1    862991\n",
      "Name: count, dtype: int64\n",
      "Resampled training data shape post SMOTE: (1725982, 212)\n"
     ]
    }
   ],
   "source": [
    "# Implement SMOTE, which is a popular technique for handling imbalanced datasets by generating synthetic samples for the minority class.\n",
    "print(\"\\n--- Handling Class Imbalance with SMOTE ---\")\n",
    "print(\"Original training target distribution:\\n\", y_train.value_counts)\n",
    "\n",
    "smote = SMOTE(random_state=42) # random_state again from 00_SciKitLearn_Intro\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nResampled training target distribution:\\n\", y_train_resampled.value_counts())\n",
    "print(f\"Resampled training data shape post SMOTE: {X_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd257c",
   "metadata": {},
   "source": [
    "## Handling Class Imbalance with SMOTE\n",
    "\n",
    "**Why Class Imbalance is a Problem:**\n",
    "\n",
    "In this credit risk prediction project, we are dealing with a significant class imbalance. As observed in the initial data loading and exploration, the number of loans that were 'Fully Paid' (non-default, class 0) is much higher than the number of loans that defaulted (class 1). If a machine learning model is trained directly on this imbalanced dataset, it will likely become biased towards the majority class (non-default). This means the model will be very good at predicting non-defaults but poor at identifying the minority class (defaults), which is the primary outcome we are interested in predicting accurately. A model that cannot effectively identify defaults is not useful for assessing credit risk.\n",
    "\n",
    "**What SMOTE Does and Why it's Applied Only to Training Data:**\n",
    "\n",
    "**SMOTE** (Synthetic Minority Over-sampling Technique) is a widely used method to address class imbalance. It works by creating synthetic examples of the minority class. Instead of simply duplicating existing minority samples, SMOTE generates new, synthetic samples that are similar to existing minority samples. It does this by selecting a minority class instance and creating new instances along the line segments joining it to its k-nearest neighbors. This helps to increase the representation of the minority class in the dataset, providing the model with more examples to learn from.\n",
    "\n",
    "SMOTE is applied **only to the training data** (`X_train`, `y_train`). It is crucial *not* to apply SMOTE to the testing data (`X_test`, `y_test`). The testing set is meant to simulate unseen, real-world data, which naturally reflects the original imbalanced distribution. Applying SMOTE to the test set would create synthetic samples that the model might have implicitly learned patterns from during training, leading to an overly optimistic and unrealistic evaluation of the model's performance. By keeping the test set in its original imbalanced state, we get a true measure of how well the trained model generalizes to real-world data.\n",
    "\n",
    "**Confirmation of Balanced Training Data:**\n",
    "\n",
    "As shown by the output after applying SMOTE, the `y_train_resampled.value_counts()` now shows that the number of samples for class 0 (non-default) and class 1 (default) are equal. This confirms that SMOTE has successfully balanced the training dataset, providing a more equitable basis for the model to learn the characteristics of both defaulted and non-defaulted loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b463f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
