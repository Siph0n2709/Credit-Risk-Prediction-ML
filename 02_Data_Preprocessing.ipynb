{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c900fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'data/accepted_2007_to_2018q4.csv/accepted_2007_to_2018q4.csv'\n",
    "df_accepted = pd.read_csv(data_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mapping = {\n",
    "    \"Charged Off\": 1,\n",
    "    \"Default\": 1,\n",
    "    \"Late (31-120 days)\": 1,\n",
    "    \"Does not meet the credit policy. Status:Charged Off\": 1,\n",
    "    \"Fully Paid\": 0,\n",
    "    \"Does not meet the credit policy. Status:Fully Paid\": 0\n",
    "} \n",
    "\n",
    "final_statuses_for_model = list(target_mapping.keys())\n",
    "df_filtered = df_accepted[df_accepted['loan_status'].isin(final_statuses_for_model)].copy()\n",
    "df_filtered['is_default'] = df_filtered['loan_status'].map(target_mapping)\n",
    "\n",
    "y = df_filtered['is_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cfd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    # 1. Identifiers and Unnecessary Columns\n",
    "    'id',\n",
    "    'member_id',\n",
    "    'url',\n",
    "    'desc',\n",
    "    'loan_status',\n",
    "    'title',\n",
    "    'zip_code',\n",
    "    'emp_title',\n",
    "\n",
    "    # 2. Data Leakage: Information that would not be available at the time of loan application, only available after the loan is issued or defaulted\n",
    "    'funded_amnt',\n",
    "    'funded_amnt_inv',\n",
    "    'pymnt_plan',\n",
    "    'out_prncp',\n",
    "    'out_prncp_inv',\n",
    "    'total_pymnt',\n",
    "    'total_pymnt_inv',\n",
    "    'total_rec_prncp',\n",
    "    'total_rec_int',\n",
    "    'total_rec_late_fee',\n",
    "    'recoveries',\n",
    "    'collection_recovery_fee',\n",
    "    'last_pymnt_d',\n",
    "    'last_pymnt_amnt',\n",
    "    'next_pymnt_d',\n",
    "    'last_credit_pull_d',\n",
    "    'last_fico_range_high',\n",
    "    'last_fico_range_low',\n",
    "\n",
    "    # 3. Hardship and Debt Settlement Information (outcomes of distress not related to the original loan application)\n",
    "    'hardship_flag',\n",
    "    'hardship_type',\n",
    "    'hardship_reason',\n",
    "    'hardship_status',\n",
    "    'deferral_term',\n",
    "    'hardship_amount',\n",
    "    'hardship_start_date',\n",
    "    'hardship_end_date',\n",
    "    'payment_plan_start_date',\n",
    "    'hardship_length',\n",
    "    'hardship_dpd',\n",
    "    'hardship_loan_status',\n",
    "    'orig_projected_additional_accrued_interest',\n",
    "    'hardship_payoff_balance_amount',\n",
    "    'hardship_last_payment_amount',\n",
    "    'debt_settlement_flag',\n",
    "    'debt_settlement_flag_date',\n",
    "    'settlement_status',\n",
    "    'settlement_date',\n",
    "    'settlement_amount',\n",
    "    'settlement_percentage',\n",
    "    'settlement_term',\n",
    "\n",
    "    # 4. Joint/Secondary Borrower Information (not relevant for single borrower analysis)\n",
    "    'annual_inc_joint',\n",
    "    'dti_joint',\n",
    "    'verification_status_joint',\n",
    "    'revol_bal_joint',\n",
    "    'sec_app_fico_range_low',\n",
    "    'sec_app_fico_range_high',\n",
    "    'sec_app_earliest_cr_line',\n",
    "    'sec_app_inq_last_6mths',\n",
    "    'sec_app_mort_acc',\n",
    "    'sec_app_open_acc',\n",
    "    'sec_app_revol_util',\n",
    "    'sec_app_open_act_il',\n",
    "    'sec_app_num_rev_accts',\n",
    "    'sec_app_chargeoff_within_12_mths',\n",
    "    'sec_app_collections_12_mths_ex_med',\n",
    "    'sec_app_mths_since_last_major_derog',\n",
    "\n",
    "    # 5. Columns with High Missing Values or Irrelevant Information\n",
    "    'mths_since_last_delinq',\n",
    "    'policy_code'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_columns_to_drop = [col for col in columns_to_drop if col in df_filtered.columns]\n",
    "X = df_filtered.drop(columns=existing_columns_to_drop + ['is_default'], errors='ignore')\n",
    "\n",
    "print(\"X and y DataFrames recreated for preprocessing.\")\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa485f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = X.copy()\n",
    "\n",
    "if 'issue_d' in X_processed.columns:\n",
    "    print(\"Processing 'issue_d' column...\")\n",
    "\n",
    "    #Convert 'issue_d' to datetime\n",
    "    X_processed['issue_d_dt'] = pd.to_datetime(X_processed['issue_d'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "    # Check for unparseable dates\n",
    "    unparseable_dates = X_processed['issue_d_dt'].isnull().sum()\n",
    "    if unparseable_dates > 0:\n",
    "        print(f\"Warning: {unparseable_dates} unparseable dates found in 'issue_d'. These will be dropped.\")\n",
    "    else:\n",
    "        print(\"All dates in 'issue_d' were successfully parsed.\")\n",
    "\n",
    "    X_processed['issue_month'] = X_processed['issue_d_dt'].dt.month\n",
    "    X_processed['issue_year'] = X_processed['issue_d_dt'].dt.year\n",
    "    X_processed['issue_dayofweek'] = X_processed['issue_d_dt'].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "    X_processed.drop(columns=['issue_d'], inplace=True, errors='ignore')\n",
    "    print(\"'issue_d' column processed and dropped.\")\n",
    "else:\n",
    "    print(\"'issue_d' column not found in the DataFrame. Skipping processing.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ed39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'earliest_cr_line' in X_processed.columns:\n",
    "    print(\"Processing 'earliest_cr_line' column...\")\n",
    "    X_processed['earliest_cr_line_dt'] = pd.to_datetime(X_processed['earliest_cr_line'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "    unparseable_cr_lines = X_processed['earliest_cr_line_dt'].isnull().sum()\n",
    "    if unparseable_cr_lines > 0:\n",
    "        print(f\"Warning: {unparseable_cr_lines} unparseable dates found in 'earliest_cr_line'. These will be dropped.\")\n",
    "    else:\n",
    "        print(\"All dates in 'earliest_cr_line' were successfully parsed.\")\n",
    "    \n",
    "    if 'issue_d_dt' in X_processed.columns:\n",
    "        X_processed['credit_history_length_months'] = ((X_processed['issue_d_dt'] - X_processed['earliest_cr_line_dt']).dt.days / 30.4375).astype(float) # Average days per month ; allow for NaN\n",
    "\n",
    "        # Handle negative values, if somehow the earliest credit line is after the issue date\n",
    "        X_processed.loc[X_processed['credit_history_length_months'] < 0, 'credit_history_length_months'] = np.nan\n",
    "        print(\"Credit history length calculated successfully.\")\n",
    "    else:\n",
    "        print(\"Warning: 'issue_d_dt' not found. Cannot calculate credit history length.\")\n",
    "    \n",
    "    X_processed.drop(columns=['earliest_cr_line', 'earliest_cr_line_dt'], inplace=True, errors='ignore')\n",
    "    print(\"'earliest_cr_line' column processed and dropped.\")\n",
    "\n",
    "else: # Almost sure this is not needed, but keeping for safety, if the column has already been split or removed (wasnt used in the original code)\n",
    "    print(\"'earliest_cr_line' column not found in the DataFrame.\")\n",
    "    \n",
    "    earliest_cr_line_encoded_cols = [col for col in X_processed.columns if col.startswith('earliest_cr_line_')] # This will be empty if the column was not processed\n",
    "\n",
    "    if earliest_cr_line_encoded_cols:\n",
    "        print(f\"Warning: The following columns were not processed due to missing 'earliest_cr_line': {earliest_cr_line_encoded_cols}\")\n",
    "\n",
    "        date_from_col_name_map = {\n",
    "            col: pd.to_datetime(col.replace('earliest_cr_line_', '').format('%b-%Y'), errors='coerce')\n",
    "            for col in earliest_cr_line_encoded_cols\n",
    "        }\n",
    "\n",
    "        X_processed['credit_history_length_months'] = np.nan\n",
    "        print(\"Calculating credit history length from encoded columns...\")\n",
    "\n",
    "        issue_d_dt = X_processed.get('issue_d_dt')\n",
    "\n",
    "        for index, row_series in X_processed.iterrows():\n",
    "            issue_date = issue_d_dt[index] if issue_d_dt is not None else pd.NaT\n",
    "\n",
    "            if pd.isna(issue_date):\n",
    "                continue\n",
    "\n",
    "            earliest_date_for_row = None\n",
    "\n",
    "            for col in earliest_cr_line_encoded_cols:\n",
    "                if row_series[col] == 1:\n",
    "                    earliest_date_for_row = date_from_col_name_map[col]\n",
    "                    break\n",
    "            if earliest_date_for_row and pd.notna(earliest_date_for_row):\n",
    "                days_diff = (issue_date - earliest_date_for_row).days\n",
    "                X_processed.loc[index, 'credit_history_length_months'] = days_diff / 30.4375  # Average days per month\n",
    "                # handle negative values\n",
    "                if X_processed.loc[index, 'credit_history_length_months'] < 0:\n",
    "                    X_processed.loc[index, 'credit_history_length_months'] = np.nan\n",
    "        \n",
    "        X_processed.drop(columns=earliest_cr_line_encoded_cols, inplace=True, errors='ignore')\n",
    "        print(\"Credit history length calculated from encoded columns.\")\n",
    "    else:\n",
    "        print(\"No encoded columns for 'earliest_cr_line' found. Cannot calculate credit history length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'issue_d_dt' in X_processed.columns:\n",
    "    X_processed.drop(columns=['issue_d_dt'], inplace=True, errors='ignore')\n",
    "    print(\"'issue_d_dt' intermediate column dropped.\")\n",
    "\n",
    "print(f\"\\nFinal shape of X_processed: {X_processed.shape}\")\n",
    "print(\"First 5 rows of X_processed:\\n\", X_processed.head())\n",
    "\n",
    "if 'issue_month' in X_processed.columns: # checking the status of NaNs after processing\n",
    "    print(f\"NaNs in 'issue_month': {X_processed['issue_month'].isnull().sum()}\")\n",
    "if 'credit_history_length_months' in X_processed.columns: \n",
    "    print(f\"NaNs in 'credit_history_length_months': {X_processed['credit_history_length_months'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transform_columns = [\n",
    "    'loan_amnt', 'installment', 'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths',\n",
    "    'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc',\n",
    "    'collections_12_mths_ex_med', 'mths_since_last_major_derog', 'acc_now_delinq',\n",
    "    'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m', 'open_act_il', 'open_il_12m',\n",
    "    'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m',\n",
    "    'open_rv_24m', 'max_bal_bc', 'all_util', 'total_rev_hi_lim', 'inq_fi',\n",
    "    'total_cu_tl', 'inq_last_12m', 'acc_open_past_24mths', 'avg_cur_bal',\n",
    "    'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt',\n",
    "    'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl',\n",
    "    'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n",
    "    'mths_since_recent_inq', 'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd',\n",
    "    'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl',\n",
    "    'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats',\n",
    "    'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m',\n",
    "    'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens',\n",
    "    'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit' # These were determined in EDA #1, with the observations of the distributions\n",
    "]\n",
    "\n",
    "print(\"\\n--- Applying Log1p Transformation to Skewed/Sparse Numerical Features ---\")\n",
    "for col in log_transform_columns:\n",
    "    if col in X_processed.columns:\n",
    "        if (X_processed[col] < 0).any().any(): # check for negative values\n",
    "            print(f\"Warning: Column '{col}' contains negative values. Skipping log transformation.\")\n",
    "        else: \n",
    "            # Must Fill NaN values with 0 before log transformation\n",
    "            # For now, we can handle them later\n",
    "            X_processed[col] = np.log1p(X_processed[col])\n",
    "            print(f\"Log1p transformation applied to column: {col}\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' not found in DataFrame. Skipping log transformation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(X_processed['annual_inc'].dropna(), kde=True) # Kernel Density Estimate True means it will plot the density curve ; annual inc looks like a normal distribution, which is much better\n",
    "plt.title('Annual Income Distribution (Log1p Transformed)')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(X_processed['delinq_2yrs'].dropna(), kde=True)\n",
    "plt.title('Delinquency in 2 Years Distribution (Log1p Transformed)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73392a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns to scale\n",
    "cols_to_scale = X_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNumerical columns to be used with StandardScalar({len(cols_to_scale)}): {cols_to_scale}...\") # print them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5674070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"\\n--- Scaling Numerical Features ---\") # for the final model, StandardScaler is used only on the training set\n",
    "X_processed[cols_to_scale] = scaler.fit_transform(X_processed[cols_to_scale])\n",
    "print(\"Scaling completed.\")\n",
    "\n",
    "print(\"\\nFirst 5 Rows of Processed DataFrame:\", X_processed.head())\n",
    "print(\"\\nDescriptive Statistics of Processed DataFrame:\\n\")\n",
    "print(X_processed[cols_to_scale].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30adc26",
   "metadata": {},
   "source": [
    "## Summary of Numerical Feature Preprocessing\n",
    "\n",
    "Based on the Exploratory Data Analysis (EDA) in the previous notebook (`01_EDA_and_Data_Loading.ipynb`), many numerical features exhibited skewed distributions or were sparse (many zero values). To address this and make the distributions more symmetrical and suitable for various machine learning models, the **log1p transformation** (log(1 + x)) was applied to the following columns:\n",
    "\n",
    "*   `loan_amnt`\n",
    "*   `installment`\n",
    "*   `annual_inc`\n",
    "*   `dti`\n",
    "*   `delinq_2yrs`\n",
    "*   `inq_last_6mths`\n",
    "*   `open_acc`\n",
    "*   `pub_rec`\n",
    "*   `revol_bal`\n",
    "*   `revol_util`\n",
    "*   `total_acc`\n",
    "*   `collections_12_mths_ex_med`\n",
    "*   `mths_since_last_major_derog`\n",
    "*   `acc_now_delinq`\n",
    "*   `tot_coll_amt`\n",
    "*   `tot_cur_bal`\n",
    "*   `open_acc_6m`\n",
    "*   `open_act_il`\n",
    "*   `open_il_12m`\n",
    "*   `open_il_24m`\n",
    "*   `mths_since_rcnt_il`\n",
    "*   `total_bal_il`\n",
    "*   `il_util`\n",
    "*   `open_rv_12m`\n",
    "*   `open_rv_24m`\n",
    "*   `max_bal_bc`\n",
    "*   `all_util`\n",
    "*   `total_rev_hi_lim`\n",
    "*   `inq_fi`\n",
    "*   `total_cu_tl`\n",
    "*   `inq_last_12m`\n",
    "*   `acc_open_past_24mths`\n",
    "*   `avg_cur_bal`\n",
    "*   `bc_open_to_buy`\n",
    "*   `bc_util`\n",
    "*   `chargeoff_within_12_mths`\n",
    "*   `delinq_amnt`\n",
    "*   `mo_sin_old_il_acct`\n",
    "*   `mo_sin_old_rev_tl_op`\n",
    "*   `mo_sin_rcnt_rev_tl_op`\n",
    "*   `mo_sin_rcnt_tl`\n",
    "*   `mort_acc`\n",
    "*   `mths_since_recent_bc`\n",
    "*   `mths_since_recent_bc_dlq`\n",
    "*   `mths_since_recent_inq`\n",
    "*   `mths_since_recent_revol_delinq`\n",
    "*   `num_accts_ever_120_pd`\n",
    "*   `num_actv_bc_tl`\n",
    "*   `num_actv_rev_tl`\n",
    "*   `num_bc_sats`\n",
    "*   `num_bc_tl`\n",
    "*   `num_il_tl`\n",
    "*   `num_op_rev_tl`\n",
    "*   `num_rev_accts`\n",
    "*   `num_rev_tl_bal_gt_0`\n",
    "*   `num_sats`\n",
    "*   `num_tl_120dpd_2m`\n",
    "*   `num_tl_30dpd`\n",
    "*   `num_tl_90g_dpd_24m`\n",
    "*   `num_tl_op_past_12m`\n",
    "*   `pct_tl_nvr_dlq`\n",
    "*   `percent_bc_gt_75`\n",
    "*   `pub_rec_bankruptcies`\n",
    "*   `tax_liens`\n",
    "*   `tot_hi_cred_lim`\n",
    "*   `total_bal_ex_mort`\n",
    "*   `total_bc_limit`\n",
    "*   `total_il_high_credit_limit`\n",
    "\n",
    "Following the log1p transformation (where applicable), **all numerical features** were scaled using `StandardScaler`. This is a crucial step for many machine learning algorithms (like those based on distance metrics or gradient descent) because it standardizes features by removing the mean and scaling to unit variance. This prevents features with larger values from dominating the learning process and helps algorithms converge faster and perform better.\n",
    "\n",
    "As confirmed by the `describe()` output after scaling, the numerical columns now have means very close to 0 and standard deviations very close to 1, indicating successful standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentage_current = (X_processed.isnull().sum() / len(X_processed)) * 100\n",
    "missing_percentage_current = missing_percentage_current[missing_percentage_current > 0].sort_values(ascending=False)\n",
    "print(\"\\nMissing Percentage of Columns in Processed DataFrame:\")\n",
    "print(missing_percentage_current.head(20)) # There are 68 columns with missing values currently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81759062",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols_nans = X_processed.select_dtypes(include=[np.number]).columns[X_processed.select_dtypes(include=[np.number]).isnull().any()].tolist()\n",
    "print(f\"\\nNumerical columns with NaN values: {numerical_cols_nans}\")\n",
    "for col in numerical_cols_nans:\n",
    "    mode_val = X_processed[col].mode()[0]  # Get the mode value\n",
    "    # X_processed[col].fillna(median_val, inplace=True)\n",
    "    X_processed[col] = X_processed[col].fillna(mode_val) # This is the same as above, but more explicit and should get rid of the warning\n",
    "    print(f\"Imputed '{col}' with mode value: {mode_val:.2f}\")\n",
    "\n",
    "# Obviously there won't be any NaN values anymore since we already filled them with the mode value (if running this code again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc5add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Check for Missing Values ---\")\n",
    "print(X_processed.isnull().sum().sum())  # Should be 0 if all NaNs are filled, but isn't since we have some categorical columns with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3beb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_to_encode = X_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\nCategorical features to be encoded ({len(categorical_features_to_encode)}): {categorical_features_to_encode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc06f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "if categorical_features_to_encode:\n",
    "    encoder_one_hot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # handle_unknown='ignore' to avoid errors with unseen categories\n",
    "\n",
    "    encoded_features = encoder_one_hot.fit_transform(X_processed[categorical_features_to_encode])\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoder_one_hot.get_feature_names_out(categorical_features_to_encode), index=X_processed.index)\n",
    "\n",
    "    X_processed = X_processed.drop(columns=categorical_features_to_encode)\n",
    "    X_processed = pd.concat([X_processed, encoded_df], axis=1)\n",
    "    print(\"\\nCategorical features encoded successfully.\")\n",
    "else:\n",
    "    print(\"\\nNo categorical features to encode.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b045396",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df # 0 means the column was not present in the original DataFrame, so it was not encoded ; 1 means the column was present and encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d75faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final X_processed Information after transformation and encoding ---\")\n",
    "X_processed.info()\n",
    "print(\"\\nTotal remaining missing values in X_processed:\", X_processed.isnull().sum().sum())  # Should be 0 if all NaNs are filled\n",
    "print(\"\\nFirst 5 rows of the final processed DataFrame:\\n\", X_processed.head())\n",
    "print(f\"\\nFinal shape of X_processed: {X_processed.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
